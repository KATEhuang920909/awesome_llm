### 大模型词表注入（Vocabulary Injection）

大模型词表注入（Vocabulary Injection）是指在预训练语言模型（如GPT、BERT等）的基础上，动态扩展其词表（Vocabulary）的技术，以适应特定任务或领域的需求。

### **一、词表注入的「原理」**

1. **词表结构与嵌入层调整**
   - 大模型的词表通常由子词（subword）或词片（token）组成（如BPE、WordPiece算法生成），每个词对应一个嵌入向量（Embedding Vector）。
   - **注入新词**时，需要扩展模型的嵌入矩阵（Embedding Matrix），新增词对应的向量，并调整模型输入层（Embedding Layer）和输出层（如LM Head）的维度。
2. **新词向量的初始化**
   - 直接随机初始化新词的嵌入向量可能导致训练不稳定。常见策略是：
     - **复用相似词向量**：例如，将新词"LLM"初始化为"language"和"model"的平均向量。
     - **对齐预训练语义空间**：通过外部词向量（如Word2Vec）映射到模型的嵌入空间。
3. **参数适配与微调**
   - 注入新词后，通常需要在小规模领域数据上对模型进行微调（Fine-tuning），使新词的嵌入向量与原有参数协同工作。
   - 某些方法（如[《Extending Pre-trained Models with Domain-Specific Vocabulary》](https://arxiv.org/abs/2104.08646））会冻结部分参数，仅训练新词相关部分以减少计算量。

------

### **二、词表注入的「原因」**

1. **解决未登录词（OOV）问题**
   - 预训练模型的词表固定，无法覆盖领域专有名词（如医学术语“EGFR”）、新造词（如网络流行语“栓Q”）或多语言词汇。
2. **提升领域任务性能**
   - 在特定领域（法律、医疗、金融）中，直接使用原始词表可能导致文本被过度切分为子词，丢失语义信息。注入领域词表可保留关键术语的完整性。
3. **多语言扩展需求**
   - 为支持新语言，需注入该语言的词汇（如中文字符、俄文字母），同时调整模型处理多语言的能力。
4. **避免全模型重训练**
   - 从头预训练大模型成本极高，词表注入允许在原有模型基础上低成本扩展，节省计算资源和时间。

------

### **三、技术挑战与解决方案**

1. **嵌入空间对齐**
   - **问题**：新词向量可能破坏原有语义空间的一致性。
   - **方案**：使用对比学习（Contrastive Learning）或跨词注意力（Cross-token Attention）对齐新旧词向量。
2. **模型结构限制**
   - **问题**：Transformer的参数量与词表大小相关，盲目扩展词表会显著增加模型体积。
   - **方案**：动态词表（Dynamic Vocabulary）、参数共享（如ALBERT的跨层参数共享）。
3. **训练数据偏差**
   - **问题**：注入新词后，若微调数据不足，模型可能过拟合或遗忘原有知识。
   - **方案**：渐进式训练（Progressive Training）或知识蒸馏（Knowledge Distillation）。

------

### **四、典型应用场景**

1. **领域适配**
   - 例如，向BERT注入法律术语后，在合同解析任务中表现更佳。
2. **多语言模型扩展**
   - 如为英文训练的GPT-2注入中文词表，支持中英混合生成。
3. **实时更新**
   - 快速响应新事件（如疫情术语“奥密克戎”）或网络热词。

------

### **五、代码及微调实验**

有两种方法：1.词表注入；2.词表训练-->添加词表

- 导入原始大模型及tokenier

```python
tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True,
    use_fast=False if model_arch == 'llama' else True
)

model = AutoModel.from_pretrained(
    model_path,
    trust_remote_code=True,
    torch_dtype=torch_dtype
)
```

- 词表注入

```python
all_words = []
with open(file, 'r', encoding='utf-8') as f:
    lines = f.readlines()
words = [line.strip() for line in lines]
all_words.extend(words)
tokenizer.add_tokens(all_words)
tokenizer.save_pretrained(save_path)
```

- 词表训练

使用sentencepiece==4.1.0 训练词表

```python
sp.SentencePieceTrainer.train(
    # 只支持 txt 和 tsv 格式
    input=corpus,
    # 保存的模型前缀名
    model_prefix='bpe_expand',
    # 词表大小
    vocab_size=vocab_size,
    # 指定模型的字符覆盖率, 中文日文等推荐为 0.9995, 其余可以尝试 1.0
    character_coverage=character_coverage,
    # 分词算法
    model_type='bpe',
    # 是否将数字划分为单个 token, 在 llama 中是这么做的
    split_digits=True if model_arch == 'llama' else False,
    # 指定在遇到未知或很少的字符时将其分解为 UTF-8 字节, 开启后等效于 bbpe
    byte_fallback=True,
    # 指定输入句子的最大长度，以字节为单位
    max_sentence_length=max_sentence_length
```

参数说明：

| **参数**              | **重要性** | **推荐场景**                                  |
| :-------------------- | :--------- | :-------------------------------------------- |
| `input`               | 必填       | 所有场景                                      |
| `model_prefix`        | 必填       | 所有场景                                      |
| `model_type`          | 高         | 根据需求选择 `unigram` 或 `bpe`               |
| `vocab_size`          | 高         | 通常设为 `32000`、`50000` 或更高              |
| `character_coverage`  | 中         | 多语言数据设为 `0.9995`，单语言数据默认 `1.0` |
| `byte_fallback`       | 中         | 处理未知字符时设为 `True`                     |
| `max_sentence_length` | 中         | 处理长文本时需调大（如 `16384`）              |
| `split_digits`        | 中         | 需区分数字时设为 `True`                       |

- 添加词表

  词表训练之后需要添加词表，并保存：

  ```python
  #1.加载bpe model
  sp_bpe = sp.SentencePieceProcessor()
  sp_bpe.load(bpe_model)
  #2.处理词汇
  raw_vocab = [sp_bpe.id_to_piece(id) for id in range(sp_bpe.get_piece_size())]
  clean_vocab = list(set(filter(is_chinese, raw_vocab)))
  #添加词汇并保存zz
  tokenizer.add_tokens(clean_vocab)
  tokenizer.save_pretrained(save_path)
  ```

- 维度更新

经过词表注入或者词表训练，两种方法其中之一之后，更新模型的维度参数：

```python
model.resize_token_embeddings(new_length)#new_length 为新词表的词汇量
model.save_pretrained(save_path)
```

- 初始化向量策略

  1. 均值初始化（常用）

  - **适用场景**：新词有明确的语义关联词（如专业术语、复合词）

  - **实现方法**：用已有相似词向量的均值初始化新词（例如，初始化“<医学诊断>”时，取“医学”和“诊断”的向量均值）。

    ```python
    # 代码示例（接续词表注入代码）
    embeddings = model.get_input_embeddings().weight.data
    token_id = tokenizer.convert_tokens_to_ids("<医学诊断>")
    similar_tokens = ["医学", "诊断", "病症"]
    similar_ids = [tokenizer.convert_tokens_to_ids(t) for t in similar_tokens]
    embeddings[token_id] = embeddings[similar_ids].mean(dim=0)
    ```

  2. 高频词初始化

  - **适用场景**：无明确关联词的新词（如新品牌名、网络流行语）

  - **实现方法**：用高频词（如 "the"、逗号等）的向量初始化，避免随机初始化带来的噪声。

    

    ```python
    # 用高频词 "," 的向量初始化
    high_freq_token = tokenizer.convert_tokens_to_ids(",")
    embeddings[token_id] = embeddings[high_freq_token].clone()
    ```

  3. 零向量初始化

  - **适用场景**：占位符或特殊标记（如 `<sep>`, `<mask>`）

  - **实现方法**：

    ```python
    embeddings[token_id] = torch.zeros_like(embeddings[token_id])
    ```

  4. 领域预训练初始化（高阶）

  - **适用场景**：大量领域新词（如医学、法律专业术语）

  - **实现方法**：

    1. 在领域语料上训练一个小型词向量模型（如 Word2Vec、FastText）。
    2. 将预训练向量对齐到大模型的嵌入空间。

    ```python
    from gensim.models import Word2Vec
    
    # 训练领域词向量
    domain_corpus = [["医学", "诊断", "病例"], ...]  # 领域文本分词后的列表
    w2v_model = Word2Vec(domain_corpus, vector_size=embeddings.shape[1])
    
    # 对齐并替换新词向量
    domain_vector = w2v_model.wv["医学诊断"]
    embeddings[token_id] = torch.tensor(domain_vector)
    ```

  5. 随机初始化

  - **适用场景**：无任何先验信息的新词（需配合后续微调）

  - **风险**：随机值可能破坏原有语义空间，导致模型混乱。

    ```python
    # 若必须使用，建议缩小初始化范围
    embeddings[token_id].normal_(mean=0, std=0.02)  # 与原模型初始化一致
    ```

  6. 多策略混合初始化

  ```python
  # 示例：50%相似词均值 + 50%高频词
  mean_embedding = embeddings[similar_ids].mean(dim=0)
  high_freq_embedding = embeddings[high_freq_token]
  embeddings[token_id] = 0.5 * mean_embedding + 0.5 * high_freq_embedding
  ```

  7. 动态初始化（训练中调整）

  ```python
  # 在训练初期逐步调整新词向量
  optimizer = torch.optim.AdamW([
      {"params": model.get_input_embeddings().parameters(), "lr": 1e-3},  # 新词高学习率
      {"params": model.other_parameters(), "lr": 1e-5}                   # 其他参数低学习率
  ])
  ```

  8. 对抗扰动增强

  ```python
  # 向初始化向量添加小噪声，提升鲁棒性
  noise = torch.randn_like(embeddings[token_id]) * 0.01
  embeddings[token_id] += noise
  ```

  ------

  #### 验证方法

  #####  **余弦相似度检查**

  ```python
  from sklearn.metrics.pairwise import cosine_similarity
  
  new_vec = embeddings[token_id].cpu().numpy().reshape(1, -1)
  ref_vecs = embeddings[similar_ids].cpu().numpy()
  similarities = cosine_similarity(new_vec, ref_vecs)
  print(f"与参考词的相似度: {similarities.mean():.3f}")  # 应 > 0.6
  ```

  #### 微调实验

  | 序号 | 词表注入策略     | 结果 |
  | ---- | ---------------- | ---- |
  | 1    | 未注入           |      |
  | 2    | 向量随机初始化   |      |
  | 3    | 向量预训练初始化 |      |
  | 4    | 向量动态初始化   |      |
  | 5    | 对抗扰动增强     |      |

  

  

  
