# MCPå®éªŒ

è¯­è¨€ï¼špython

æ¡†æ¶ï¼šfastmcp==2.10.3ã€zhipuai==2.1.5.20250611ã€fastapi==0.115.12ã€transformers==4.53.0

## server

1.åˆ›å»ºä¸€ä¸ªæ—¶é—´åœ°ç‚¹è·å–çš„serverï¼Œå‘½åä¸ºlocal_time_server.py

```python
import json
from fastmcp import FastMCP
import requests
import datetime
mcp = FastMCP("LocalTime",port=8002)
@mcp.tool(name="get_current_time",description="è·å–å½“å‰æ—¶é—´",)
def get_current_time():
    """è·å–å½“å‰æ—¶é—´å¹¶è¿›è¡Œæ ¼å¼åŒ–å±•ç¤º:return:"""
    now = datetime.datetime.now()
    formatted_time = now.strftime("%Y-%m-%d %H:%M:%S")
    return formatted_time

@mcp.tool(name="get_location",description="è·å–å½“å‰åœ°ç‚¹",)
def get_location():
    """è·å–å½“å‰åœ°ç‚¹
    :return:"""
    try:
        response = requests.get("http://ip-api.com/json/")
        data = response.json()
        if data["status"] =="success":
            location_info = {"country": data.get("country",""),
                             "region": data.get("regionName",""),
                             "city": data.get("city","")}
            return json.dumps(location_info, ensure_ascii=False)
        else:
            return json.dumps({"error":"æ— æ³•è·å–åœ°ç†ä½ç½®"}, ensure_ascii=False)
    except Exception as e:
        return json.dumps({"error":str(e)}, ensure_ascii=False)
```

2.åˆ›å»ºä¸€ä¸ªæ•°æ®åº“è¯»å–serverï¼Œå‰ææ˜¯å¼€æ•°æ®åº“æœåŠ¡ã€‚è„šæœ¬å‘½åä¸ºdatabase_server.py

```python
# -*- coding: utf-8 -*-

from pydantic import BaseModel, Field
from typing import TypedDict
from fastmcp import FastMCP
import pymysql

database_opt_mcp = FastMCP("æ•°æ®åº“æ“ä½œæœåŠ¡",port=8002)


@database_opt_mcp.tool()
def get_data_from_database(sql_instruct: str):
    # "SELECT * FROM sys_menu"
    """è·å¾—æ•°æ®åº“çš„æ•°æ®"""
    db = pymysql.connect(host="127.0.0.1", user="root", password="Advanced@1992", database="mcp_test")
    cursor = db.cursor()
    cursor.execute("SELECT * FROM sys_menu")

    result = cursor.fetchall()
    for row in result:
        print(row)
    desc = cursor.description

    """Get various statistics"""
    return {"count": len(result), "columns": [k[0] for k in desc]}



if __name__ == '__main__':
    database_opt_mcp.run(transport="streamable-http")
```

3.åˆ›å»ºä¸€ä¸ªå¤©æ°”é¢„æŠ¥serverï¼Œå‘½åä¸ºweather_server.py

```python
# -*- coding: utf-8 -*-

from pydantic import BaseModel, Field
from typing import TypedDict
from fastmcp import FastMCP
import pymysql

mcp_weather = FastMCP("å¤©æ°”é¢„æŠ¥æœåŠ¡",port=8001)


# Using Pydantic models for rich structured data
class WeatherData(BaseModel):
    temperature: float = Field(description="Temperature in Celsius")
    humidity: float = Field(description="Humidity percentage")
    condition: str
    wind_speed: float


@mcp_weather.tool()
def get_weather(city: str) -> WeatherData:
    """è·å–ç»“æ„åŒ–çš„å¤©æ°”æ•°æ®"""
    return WeatherData(
        temperature=22.5, humidity=65.0, condition="partly cloudy", wind_speed=12.3
    )


# Using TypedDict for simpler structures
class LocationInfo(TypedDict):
    latitude: float
    longitude: float
    name: str


@mcp_weather.tool(description="è·å–ä½ç½®åæ ‡")
def get_location(address: str) -> LocationInfo:
    return LocationInfo(latitude=51.5074, longitude=-0.1278, name="London, UK")


# Using dict[str, Any] for flexible schemas
@mcp_weather.tool()
def get_statistics(data_type: str) -> dict[str, float]:
    """è·å–å„ç§æ•°æ®"""
    return {"mean": 42.5, "median": 40.0, "std_dev": 5.2}


# Ordinary classes with type hints work for structured output
class UserProfile:
    name: str
    age: int
    email: str | None = None

    def __init__(self, name: str, age: int, email: str | None = None):
        self.name = name
        self.age = age
        self.email = email


@mcp_weather.tool()
def get_user(user_id: str) -> UserProfile:
    """è·å–ç”¨æˆ·é…ç½®æ–‡ä»¶"""
    return UserProfile(name="Alice", age=30, email="alice@example.com")


# Classes WITHOUT type hints cannot be used for structured output
class UntypedConfig:
    def __init__(self, setting1, setting2):
        self.setting1 = setting1
        self.setting2 = setting2


# Lists and other types are wrapped automatically
@mcp_weather.tool()
def list_cities(city_list) -> list[str]:
    """è·å–åŸå¸‚åˆ—è¡¨"""
    return ["London", "Paris", "Tokyo"]
    # Returns: {"result": ["London", "Paris", "Tokyo"]}


@mcp_weather.tool()
def get_temperature(city: str) -> float:
    """è·å–æ¸©åº¦å€¼"""
    return 22.5
    # Returns: {"result": 22.5}


if __name__ == '__main__':
    mcp_weather.run(transport="streamable-http")
```

serveråˆ›å»ºæ­¥éª¤ï¼š

1. å¯¼å…¥fasetmcpç­‰å¿…è¦ä¾èµ–åº“ï¼›
2. åˆå§‹åŒ–ä¸€ä¸ªFastMCPå®ä¾‹ï¼Œè®¾å®šå¯¹è±¡åï¼Œç«¯å£åï¼›
3. å®šä¹‰è£…é¥°å™¨åŠç›¸åº”å‡½æ•°ï¼Œå®šä¹‰å·¥å…·åç§°ã€å·¥å…·æè¿°ã€å·¥å…·å¯ç”¨ç¦ç”¨ç­‰ï¼›
4. å¯åŠ¨mcpç¤ºä¾‹ï¼Œå®šä¹‰é€šä¿¡åè®®ï¼Œå¯é€‰ä¸º ["stdio"ï¼Œ "streamable-http"]ï¼Œå…¶ä¸­stdioä¸ºæœ¬åœ°é€šä¿¡ï¼Œstreamable-httpä¸ºhttpé€šä¿¡

åˆ†æï¼š

1. æ ¼å¼å®Œå…¨ç›¸åŒ

2. åŒæ—¶å¼€å¯ä»¥ä¸Šä¸‰ä¸ªæœåŠ¡ï¼Œç«¯å£åˆ†åˆ«ä¸º8000ï¼Œ8001ï¼Œ8002

## client

### ç®€å•è°ƒç”¨

```python
import asyncio
from fastmcp import Client

client = Client("http://127.0.0.1:8002/mcp/")

async def call_tool(tool_name: str, *args) -> str:
    """Call a tool by name with given arguments."""
    result = await client.call_tool(tool_name, *args)
    print(f"{tool_name}({', '.join(map(str, args))}) = {result}")

async def run():
    """Run the client and call tools."""

    async with client:
        tools = await client.list_tools()
        print(f"Available tools: {', '.join(tool.name for tool in tools)}")

        await call_tool("get_current_time", {})

if __name__ == "__main__":
    asyncio.run(run())

#  æ‰“å°ç»“æœ
#tools [Tool(name='get_current_time', title=None, description='è·å–å½“å‰æ—¶é—´', inputSchema={'properties': {}, 'type': 'object'}, outputSchema=None, annotations=None, meta=None), Tool(name='get_location', title=None, description='è·å–å½“å‰åœ°ç‚¹', inputSchema={'properties': {}, 'type': 'object'}, outputSchema=None, annotations=None, meta=None)]
#Available tools: get_current_time, get_location
#get_current_time({}) = CallToolResult(content=[TextContent(type='text', text='2025-07-11 16:19:24', annotations=None, meta=None)], structured_content=None, data=None, is_error=False)

```

1. è¿æ¥8002ç«¯å£ï¼›
2. client.list_toolsï¼šåˆ—å‡ºå¯¹åº”ç«¯å£ä¸‹çš„å·¥å…·åŠå‚æ•°ï¼›
3. client.call_toolï¼šä¼ å…¥å·¥å…·åç§°ã€å‚æ•°ï¼Œè¿”å›å·¥å…·è¿è¡Œç»“æœï¼›

### ç»“åˆLLMå·¥å…·è¯†åˆ«

#### åŸºäºZhiPuAI apiè°ƒç”¨

```python
import asyncio
import json
import logging
import os
import shutil
from contextlib import AsyncExitStack
from datetime import timedelta
from typing import Any

from mcp import Tool, StdioServerParameters, stdio_client
from mcp.client.session import ClientSession
from mcp.client.streamable_http import streamablehttp_client
from zhipuai import ZhipuAI
llm_client = ZhipuAI(api_key="**********")  # è¯·å¡«å†™æ‚¨è‡ªå·±çš„APIKey
```

1. toolsæ ¼å¼è½¬æ¢

```python
def convert_mcp_to_llm_tools(mcp_tools: list) -> list:
    """å°†MCP Serverè¿”å›çš„å·¥å…·åˆ—è¡¨è½¬æ¢ä¸ºZhiPuAIå‡½æ•°è°ƒç”¨æ ¼å¼"""

    llm_tools = []

    for tool in mcp_tools:
        tool_schema = {
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "parameters": {}
            }
        }

        input_schema = tool.inputSchema
        print("input_schema",input_schema)
        parameters = {
            "type": input_schema['type'],
            "properties": input_schema['properties'],
            "required": input_schema['required'] if "required" in input_schema else [],
            "additionalProperties": False
        }
        for prop in parameters["properties"].values():
            # ç‰¹æ®Šå¤„ç†æšä¸¾å€¼
            if "enum" in prop:
                prop["description"] = f"å¯é€‰å€¼: {', '.join(prop['enum'])}"

        tool_schema["function"]["parameters"] = parameters
        llm_tools.append(tool_schema)
    return llm_tools
```

2.å»ºç«‹Serverç®¡ç†ç±»ï¼š

```python
class Server:
    """ç®¡ç†æ‰€æœ‰MCP Serverçš„è¿æ¥å’Œå·¥å…·æ‰§è¡Œ"""

    def __init__(self, name: str, config: dict[str, Any]) -> None:
        self.name: str = name
        self.config: dict[str, Any] = config
        self.session: ClientSession | None = None
        self._cleanup_lock: asyncio.Lock = asyncio.Lock()
        self.exit_stack: AsyncExitStack = AsyncExitStack()

    async def initialize(self) -> None:
        """åˆå§‹åŒ–æ‰€æœ‰ MCP Server"""
        try:
            # streamable-http æ–¹å¼
            if "type" in self.config and self.config["type"] == "streamable-http":
                streamable_http_transport = await self.exit_stack.enter_async_context(
                    streamablehttp_client(
                        url=self.config["url"],
                        timeout=timedelta(seconds=60)
                    )
                )
                read_stream, write_stream, _ = streamable_http_transport
                session = await self.exit_stack.enter_async_context(
                    ClientSession(read_stream, write_stream)
                )
                await session.initialize()
                self.session = session
            # stdio æ–¹å¼
            if "command" in self.config and self.config["command"]:
                command = (
                    shutil.which("npx")
                    if self.config["command"] == "npx"
                    else self.config["command"]
                )
                server_params = StdioServerParameters(
                    command=command,
                    args=self.config["args"],
                    env={**os.environ, **self.config["env"]}
                    if self.config.get("env")
                    else None,
                )
                stdio_transport = await self.exit_stack.enter_async_context(
                    stdio_client(server_params)
                )
                read, write = stdio_transport
                session = await self.exit_stack.enter_async_context(
                    ClientSession(read, write)
                )
                await session.initialize()
                self.session = session
            print(f"ğŸ”— è¿æ¥MCPæœåŠ¡ {self.name}...")
        except Exception as e:
            logging.error(f"âŒ åˆå§‹åŒ–é”™è¯¯ {self.name}: {e}")
            await self.cleanup()
            raise

    async def list_tools(self) -> list[Tool]:
        """ä»MCP Serveråˆ—å‡ºæ‰€æœ‰å·¥å…·"""
        if not self.session:
            raise RuntimeError(f"Server {self.name} not initialized")

        tools_response = await self.session.list_tools()
        return tools_response.tools

    async def execute_tool(
            self,
            tool_name: str,
            arguments: str,
            retries: int = 2,
            delay: float = 1.0,
    ) -> str | None:
        """æ‰§è¡Œå·¥å…·"""
        if not self.session:
            raise RuntimeError(f"Server {self.name} not initialized")
        arguments = json.loads(arguments) if arguments else {}
        attempt = 0
        while attempt < retries:
            try:
                print(f"Executing {tool_name}...")
                result = await self.session.call_tool(tool_name, arguments)
                if result.isError:
                    print(f"Tool error: {result.error}")
                print(f"\nğŸ”§ Tool '{tool_name}' result: {result.content[0].text}")
                return result.content[0].text
            except Exception as e:
                attempt += 1
                print(
                    f"Error executing tool: {e}. Attempt {attempt} of {retries}."
                )
                if attempt < retries:
                    print(f"Retrying in {delay} seconds...")
                    await asyncio.sleep(delay)
                else:
                    logging.error("Max retries reached. Failing.")
                    raise
        return None

    async def cleanup(self) -> None:
        async with self._cleanup_lock:
            try:
                await self.exit_stack.aclose()
                self.session = None
            except Exception as e:
                logging.error(f"Error during cleanup of server {self.name}: {e}")
```

3.å»ºç«‹Clientç±»ï¼š

```python
class Client:

    def __init__(self, servers: list[Server]):
        self.servers: list[Server] = servers
        self.llm_tools: list[dict] = []

    async def cleanup_servers(self) -> None:
        for server in reversed(self.servers):
            try:
                await server.cleanup()
            except Exception as e:
                print(f"Warning during final cleanup: {e}")

    async def get_response(self, messages) :
        """æäº¤LLMï¼Œå¹¶è·å–å“åº”"""
        try:
            # completion = llm_client.chat.completions.create(
            #     model="qwen3_32",
            #     messages=messages,
            #     tools=self.llm_tools,
            #     tool_choice="auto"
            # )

            response = llm_client.chat.completions.create(
                model="glm-4-plus",  # è¯·å¡«å†™æ‚¨è¦è°ƒç”¨çš„æ¨¡å‹åç§°
                messages=messages,
                tools=self.llm_tools,
                tool_choice="auto",
                stream=False,
                max_tokens=4096,
                temperature=0.8
            )
            # print(response)
            # for chunk in response:
            #     cont = chunk.choices[0].delta.content
            #     full_response += cont

            return response.choices[0].message

        except Exception as e:
            error_message = f"Error getting LLM response: {str(e)}"
            logging.error(error_message)
            return None

    async def start(self):
        """å¼€å§‹MCP Client"""
        for server in self.servers:
            # print(server)
            try:
                await server.initialize()
            except Exception as e:
                logging.error(f"Failed to initialize server: {e}")
                await self.cleanup_servers()
                return
        all_tools = []
        for server in self.servers:
            tools = await server.list_tools()
            all_tools.extend(tools)
        # å°†æ‰€æœ‰å·¥å…·è½¬ä¸ºllmæ ¼å¼
        self.llm_tools = convert_mcp_to_llm_tools(all_tools)
        print("self.llm_tools",self.llm_tools)
        # exit()
        await self.chat_loop()

    async def run(self, messages: list[Any], tool_call_count: int = 0, max_tools: int = 5):
        """è·å–LLMå“åº”"""

        llm_response = await self.get_response(messages)
        # print("llm_response",llm_response)
        result = await self.process_llm_response(llm_response)
        if tool_call_count >= max_tools:
            # å¼ºåˆ¶ç»“æŸå¹¶è¿”å›æç¤ºä¿¡æ¯
            messages.append({
                "role": "assistant",
                "content": "å·²è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°é™åˆ¶"
            })
        else:
            messages.append(result)
        return messages, result

    async def chat_loop(self):
        system_message = (
            "ä½ æ˜¯ä¸€ä¸ªå¸®åŠ©äººçš„AIåŠ©æ‰‹ã€‚"
        )
        messages = [{"role": "system", "content": system_message}]
        tool_call_count = 0
        while True:
            try:
                user_input = input("ğŸ‘¨â€ğŸ’»: ").strip().lower()
                if user_input in ["quit"]:
                    print("\nExiting...")
                    break
                messages.append({"role": "user", "content": user_input})

                messege, result = await self.run(messages, tool_call_count)
                if result["role"] == "tool":
                    # await self.run(messages, tool_call_count)
                    tool_call_count += 1
                reply = messege[-1]["content"]
                print(f"\n ğŸ¤– : {reply}")
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Goodbye!")
                break
            except EOFError:
                break

    async def process_llm_response(self, llm_response) -> dict:
        """"""
        tool_call = llm_response.tool_calls
        print("tool_call", tool_call)
        if tool_call:
            tool_call = tool_call[0].function
            print(f"Executing tool: {tool_call.name}")
            print(f"With arguments: {tool_call.arguments}")
            for server in self.servers:
                tools = await server.list_tools()
                print("tools",tools)
                if any(tool.name == tool_call.name for tool in tools):
                    try:
                        result = await server.execute_tool(tool_call.name, tool_call.arguments)
                        print(f"Tool execution result: {result}")
                        return {"role": "tool", "content": result}
                    except Exception as e:
                        error_msg = f"Error executing tool: {str(e)}"
                        logging.error(error_msg)
        return {"role": "assistant", "content": llm_response.content}
```

4.ä¿®æ”¹ç›¸å…³å‚æ•°

```json
{
  "mcpServers": {
    "bi-server": {
      "type": "streamable-http",
      "url": "http://127.0.0.1:8000/mcp"
    },
    "weather-server": {
      "type": "streamable-http",
      "url": "http://127.0.0.1:8001/mcp"
    },
    "local-time-server": {
      "type": "streamable-http",
      "url": "http://127.0.0.1:8002/mcp"
    }
  }
}
```

5.åˆ›å»ºåˆå§‹åŒ–å‡½æ•°

```python
async def main():
    # è¯»å–mcp serveré…ç½®æ–‡ä»¶
    with open("config.json", "r") as f:
        config = json.load(f)
    servers = [
        Server(name, srv_config)
        for name, srv_config in config["mcpServers"].items()
    ]
    print("ğŸš€ Simple MCP Client")
    client = Client(servers)
    await client.start()
```

6.åˆ›å»ºå…¥å£å‡½æ•°

```python
def cli():
    """CLI entry point for uv script."""
    asyncio.run(main())
```

è¿è¡Œç»“æœï¼š

![image-20250711174215643](D:\code\llm\awesome_llm\pic\image-20250711174215643.png)

#### åŸºäºFastAPI çš„LLMè°ƒç”¨

1.åŸºäºfastapiå¼€å¯LLMæœåŠ¡ï¼š

```python
import os
from starlette.responses import StreamingResponse
from transformers import TextIteratorStreamer
from threading import Thread
from fastapi import FastAPI, Request
from transformers import AutoTokenizer, AutoModelForCausalLM
import uvicorn
import json
import datetime
import torch
# åˆ›å»ºä¸€ä¸ªFastAPIåº”ç”¨ç¨‹åºå®ä¾‹
app = FastAPI()

model_name = r"**************"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)


#
@app.post("/")
async def create_item(request: Request):
    global model, tokenizer  # å£°æ˜å…¨å±€å˜é‡ä»¥ä¾¿åœ¨å‡½æ•°å†…éƒ¨ä½¿ç”¨æ¨¡å‹å’Œåˆ†è¯å™¨
    json_post_raw = await request.json()  # è·å–POSTè¯·æ±‚çš„JSONæ•°æ®
    json_post = json.dumps(json_post_raw)  # å°†JSONæ•°æ®è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    json_post_list = json.loads(json_post)  # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºPythonå¯¹è±¡
    msg = json_post_list.get('contents')  # è·å–è¯·æ±‚ä¸­çš„æç¤º
    max_length = json_post_list.get('max_length')  # è·å–è¯·æ±‚ä¸­çš„æœ€å¤§é•¿åº¦
    print(msg)
    # æ„å»ºè¾“å…¥
    input_tensor = tokenizer.apply_chat_template(msg, add_generation_prompt=True, return_tensors="pt")
    # é€šè¿‡æ¨¡å‹è·å¾—è¾“å‡º
    outputs = model.generate(input_tensor.to(model.device), max_new_tokens=max_length)
    result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)
    now = datetime.datetime.now()  # è·å–å½“å‰æ—¶é—´
    time = now.strftime("%Y-%m-%d %H:%M:%S")  # æ ¼å¼åŒ–æ—¶é—´ä¸ºå­—ç¬¦ä¸²
    # æ„å»ºå“åº”JSON
    answer = {
        "response": result,
        "status": 200,
        "time": time
    }
    # æ„å»ºæ—¥å¿—ä¿¡æ¯
    log = "[" + time + "] " + '", prompt:"' + msg[-1]["content"] + '", response:"' + repr(result) + '"'
    print(log)  # æ‰“å°æ—¥å¿—

    # torch_gc()  # æ‰§è¡ŒGPUå†…å­˜æ¸…ç†    return answer  # è¿”å›å“åº”
    return result


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=int(os.environ.get("API_PORT", 7777)), workers=1)
```

2. toolè¯†åˆ«å’Œè½¬æ¢æ–¹å¼

   å·¥å…·è¯†åˆ«æ–¹å¼ç›¸åŒï¼Œæ ¼å¼è½¬æ¢æ–¹å¼æ›´çµæ´»åªéœ€æ–¹ä¾¿åç»­promptè¯»å–å’Œä½¿ç”¨å³å¯ï¼š

   ```python
   def convert_mcp_to_llm_tools(mcp_tools: list) -> list:
       """å°†MCP Serverè¿”å›çš„å·¥å…·åˆ—è¡¨è½¬æ¢ä¸ºLLMè°ƒç”¨æ ¼å¼"""
   
       llm_tools = []
   
       for tool in mcp_tools:
           tool_schema = {"name": tool.name,
                          "description": tool.description,
                          "arguments": tool.inputSchema['required'] if "required" in tool.inputSchema else [],
                          }
   
           llm_tools.append(tool_schema)
       return llm_tools
   ```

3. Serverç«¯ç®¡ç†ç±»å’Œä¸Šä¸€èŠ‚æ–¹å¼ç›¸åŒï¼Œæ­¤å¤„æš‚ä¸èµ˜è¿°ï¼›

4. å»ºç«‹Clientç±»

å¦‚ä¸‹å‡ ä¸ªå‡½æ•°æœ‰æ‰€åŒºåˆ«ï¼š

* è°ƒç”¨å¤§æ¨¡å‹

```python
async def get_response(self, messages):
    """æäº¤LLMï¼Œå¹¶è·å–å“åº”"""

    # tool_desc_name = {unit["description"]: (unit["name"], unit["arguments"]) for unit in self.llm_tools}
    tool_name = {k: self.tool_desc_name[k][0] for k in self.tool_desc_name}
    tools_prompt = f"å·¥å…·åç§°åŠå¯¹åº”çš„æè¿°åˆ—è¡¨å¦‚ä¸‹ï¼š{tool_name}"
    post_prompt = "è¯·é€‰æ‹©å‡ºä¸queryæœ€åŒ¹é…çš„å·¥å…·ï¼Œè¿”å›jsonæ ¼å¼ä¸ºï¼š{'tool_name':å·¥å…·åç§°}ï¼Œå¦‚æ²¡æœ‰åŒ¹é…å·¥å…·ï¼Œè¿”å›{'tool_name':None}\n"
    prompt = f"queryå¦‚ä¸‹ï¼šã€{messages[-1]['content']}ã€‘\n" + post_prompt + tools_prompt
    messages[-1]['content'] = prompt
    print("messages", messages)
    try:
        payload = {
            "contents": messages,
            "max_length": 500,
            "temperature": 0.8
        }
        response = requests.post(url_api, json=payload)
        result = response.json()
        result = result.split('</think>')[-1].strip()
        print("result", result)
        return result
    except Exception as e:
        error_message = f"Error getting LLM response: {str(e)}"
        logging.error(error_message)
        return None
```

* å¤§æ¨¡å‹ç»“æœåå¤„ç†

  ```python
  def extract_dict_with_regex(self, input_str: str) -> dict:
      """
      ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»å­—ç¬¦ä¸²ä¸­æå–å­—å…¸
  
      å‚æ•°:
          input_str (str): åŒ…å«å­—å…¸çš„å­—ç¬¦ä¸²
  
      è¿”å›:
          dict: æå–å‡ºçš„å­—å…¸å¯¹è±¡
      """
      # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…Pythonå­—å…¸
      # åŒ¹é…æ¨¡å¼ï¼š{å¼€å¤´ï¼Œ}ç»“å°¾ï¼Œä¸­é—´åŒ…å«ä»»æ„å­—ç¬¦ï¼ˆéè´ªå©ªåŒ¹é…ï¼‰
      pattern = r'\{.*?\}'
      match = re.search(pattern, input_str, re.DOTALL)
  
      if not match:
          raise ValueError("æœªåœ¨å­—ç¬¦ä¸²ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„å­—å…¸ç»“æ„")
  
      # æå–åŒ¹é…çš„å­—å…¸å­—ç¬¦ä¸²
      dict_str = match.group(0)
  
      # å°†å•å¼•å·è½¬æ¢ä¸ºåŒå¼•å·ä½¿å…¶ç¬¦åˆJSONæ ¼å¼
      # æ³¨æ„ï¼šä»…è½¬æ¢é”®å’Œå€¼å‘¨å›´çš„å¼•å·ï¼Œä¸è½¬æ¢å†…å®¹ä¸­çš„å•å¼•å·
      json_str = re.sub(r"'(.*?)'", r'"\1"', dict_str)
  
      # è§£æJSONå­—ç¬¦ä¸²
      return json.loads(json_str)
  ```

* toolè°ƒç”¨

```python
async def process_llm_response(self, llm_response) -> dict:
    """"""
    # {'tool_name': å·¥å…·åç§°}
    llm_response = self.extract_dict_with_regex(llm_response)
    tool_call = llm_response["tool_name"]
    # self.tool_desc_name = {unit["description"]: (unit["name"], unit["arguments"]) for unit in self.llm_tools}
    # print("tool_call", tool_call)
    if tool_call:
        print(f"Executing tool: {tool_call}")
        print(f"With arguments: {self.tool_desc_name[tool_call][1]}")
        for server in self.servers:
            tools = await server.list_tools()
            for tool in tools:
                print("tool", tool.name)
            if any(tool.name == tool_call for tool in tools):
                try:
                    result = await server.execute_tool(tool_call, self.tool_desc_name[tool_call][1])
                    print(f"Tool execution result: {result}")
                    return {"role": "tool", "content": result}
                except Exception as e:
                    error_msg = f"Error executing tool: {str(e)}"
                    logging.error(error_msg)
    return {"role": "assistant", "content": llm_response.content}
```

è¯†åˆ«ç»“æœï¼š

![image-20250712180632143](D:\code\llm\awesome_llm\pic\image-20250712180632143.png)

æ—¶é—´ä»“ä¿ƒï¼Œéš¾å…æœ‰çº°æ¼ï¼Œæ¬¢è¿æ‰¹è¯„æŒ‡æ­£ï¼Œäº’ç›¸è®¨è®ºã€‚