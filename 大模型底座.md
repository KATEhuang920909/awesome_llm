##  ChatGLM

### 基本任务：

**自回归填充任务** ，可以完成NLU、NLG等任务。

自回归空白填充实现原理：

![img](https://pic4.zhimg.com/80/v2-46a1ed6d2f7a83d399f818503383010f_720w.webp)



原理解析：

1.随机采样文本片段，片段长度服从k=3 的泊松分布，直至15%的文本被mask掉。这样文本被分割成了PartA和PartB两部分。

2.将PartA和打乱后的PartB拼接在一起，PartB的每一个片段前后加上[S]和[E]的起始和终止符，并用二维的位置编码表示文本的位置关系。第一维描述PartA和PartB片段的对应位置关系；第二维描述PartA和PartB的片段之间的分割关系。

3.自注意力掩码控制预测目标可以看到的上下文信息。

### 模型结构

1.使用decoder-only结构：将Part A输入模型并编码成特征向量，然后再依次预测Part B中每个片段的每个token。

2.结构调整：

1. 重新排列了LN和残差连接的顺序，具体来讲就是将Post-LN改成Pre-LN。
2. 使用一个线性层来预测输出词；
3. 将ReLU激活函数替换为GeLU激活函数。

### 多任务训练

1.片段掩码；

2.句子掩码；

3.篇章掩码；

## LLama



![img](https://pic1.zhimg.com/v2-f9377c234f02c6bff3c4d1a065f1c248_r.jpg)

仍然用到了decoder-only的结构

改进点：

1.Multi-Head Attention ==>GQA   ，即在多头的基础上将Q分组，共享一组kv

2.Positional-Encoding ==> RoPE（相对位置编码）

3.Layer-Normalization ==> RMSNorm 

RMSNorm计算公式：
$$
a_{i}^{'} = \frac{a_i}{RMS(a)}g_i ， 其中RMS(a) = \sqrt {\frac{1}{n}\sum_{i=1}^{n} a_{i}^{2}}
$$
4.token:BPE

## LLama2

相比于Llama1：

1.训练数据多出40%；

2.上下文长度翻倍；

3.预训练token为2万亿；



## BLOOM

结构：decoder-only结构

优化点：

![bloom_architecture](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b48ae4e011bd4250ab56f5fb29120ea6~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

1.Alibi Positional Embeddings

2.Embedding LayerNorm